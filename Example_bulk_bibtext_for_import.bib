@inproceedings{2015_LongJ_etal_FullyConvolutionalNetworks,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3431--3440},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298965},
  urldate = {2024-02-15},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Convolutional Neural Network\2015 Fully_convolutional_networks_for_semantic_segmentation.pdf}
}

@inproceedings{2015_SzegedyC_etal_GoingDeeperConvolutions,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2024-02-15},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Convolutional Neural Network\2015 Going_deeper_with_convolutions.pdf}
}

@inproceedings{2016_HeK_etal_DeepResidualLearning,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2024-01-16},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Deep Learning\2016 Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{2016_ZagoruykoS_KomodakisN_WideResidualNetworks,
  title = {Wide {{Residual Networks}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2016},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2016},
  pages = {87.1-87.12},
  publisher = {British Machine Vision Association},
  address = {York, UK},
  doi = {10.5244/C.30.87},
  urldate = {2024-02-15},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.},
  isbn = {978-1-901725-59-9},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Convolutional Neural Network\2016 WIde Residual Networks.pdf}
}

@inproceedings{2017_HuangG_etal_DenselyConnectedConvolutional,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {2261--2269},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  urldate = {2024-02-15},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Convolutional Neural Network\2017 Densely_Connected_Convolutional_Networks.pdf}
}

@inproceedings{2017_IsolaP_etal_ImagetoImageTranslationConditional,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2017},
  month = jul,
  pages = {5967--5976},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.632},
  urldate = {2024-02-12},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  annotation = {Nickname: Pix2pix, PatchGAN},
  file = {C:\Users\elvin\Documents\Papers\Image Translation\2017 Image-to-Image_Translation_with_Conditional_Adversarial_Networks.pdf}
}

@inproceedings{2017_ZhuJY_etal_UnpairedImagetoImageTranslation,
  title = {Unpaired {{Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  month = oct,
  pages = {2242--2251},
  publisher = {IEEE},
  address = {Venice},
  doi = {10.1109/ICCV.2017.244},
  urldate = {2024-05-18},
  isbn = {978-1-5386-1032-9},
  annotation = {Nickname: CycleGAN},
  file = {C:\Users\elvin\Zotero\storage\J4AHWE3F\Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf}
}

@inproceedings{2018_HuJ_etal_SqueezeandExcitationNetworks,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00745},
  urldate = {2024-02-15},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the ``Squeezeand-Excitation'' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-ofthe-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a {$\sim$}25\% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\elvin\Documents\Papers\Convolutional Neural Network\2018 Squeeze-and-Excitation_Networks.pdf}
}
